{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ebcb1a",
   "metadata": {},
   "source": [
    "**TAU02E** - **Tomi Maijala** (LUT), **Vinh Van** (TAU)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The quantification of foliar elements within vegetation canopies—specifically the counting of leaves and the estimation of leaf area—stands as a central problem in contemporary remote sensing, forestry, and precision agriculture. Leaves function as the primary biological interface for gas exchange, driving the essential processes of photosynthesis, transpiration, and carbon sequestration. Consequently, the ability to derive precise, spatially explicit metrics of leaf distribution is not merely an academic exercise but a fundamental requirement for robust ecological modeling, crop yield forecasting, and the monitoring of forest health in the face of climate change.\n",
    "\n",
    "Historically, the assessment of canopy structure relied heavily on direct, destructive sampling or passive optical methods. Destructive techniques, while accurate, are labor-intensive, site-specific, and impossible to scale; optical methods such as hemispherical photography or localized light sensors provide valuable estimates of Leaf Area Index (LAI) but are inherently limited by their two-dimensional nature and saturation issues in dense canopies. [\\[1\\]](https://cid-inc.com/blog/what-affects-leaf-area-index-estimation-accuracy-in-field-and-remote-methods/) The\n",
    "emergence of Light Detection and Ranging (LiDAR) technology has fundamentally altered this\n",
    "landscape. As an active remote sensing modality capable of penetrating canopy gaps and\n",
    "recording multiple returns, LiDAR offers a unique capacity to digitize the three-dimensional\n",
    "(3D) architecture of vegetation with millimeter-level precision.\n",
    "\n",
    "However, the transition from raw LiDAR point clouds to meaningful biological integers—such\n",
    "as a \"leaf count\"—is fraught with complexity. It requires navigating the \"semantic gap\"\n",
    "between geometric coordinates and biological organs. This challenge necessitates a\n",
    "sophisticated interplay of data acquisition strategies, noise filtration, semantic segmentation\n",
    "(differentiating wood from leaf), and instance segmentation (distinguishing individual leaves).\n",
    "\n",
    "## Dataset\n",
    "This report demonstrates a workflow for leaf counting and area estimation using 3D point cloud data acquired via terrestrial LiDAR scanning. For the Basic Course on Mathematical Modelling final project, we were given LiDAR data of a graphically simulated tree. The point cloud has been measured by LiDAR from [0 0 1.5]. The LiDAR is able to measure from the same direction only the object closest to it. The measured point can be a leaf, tree branch, tree trunk, or ground surface. There are no obstacles to the line connecting the measured point and the laser, the point [0 0 1.5]. The data is obtained from [HELIOS++](https://www.geog.uni-heidelberg.de/en/3dgeo/projects-of-the-3dgeo-research-group/helios). \n",
    "\n",
    "This dataset only contains the 3D coordinates (X, Y, Z) of the points in the point cloud. So, there is no ground truth information for semantic segmentation\n",
    "(differentiating wood from leaf), or instance segmentation (distinguishing individual leaves), or total leaf area or leaf count. Therefore, we will rely mainly on qualitative analysis and comparison with simulated data from HELIOS++ to assess the reliability and accuracy of our methods.\n",
    "\n",
    "![Tree Point Cloud](./dataset.png)\n",
    "\n",
    "Here you can see that the point cloud is denser on the side facing the LiDAR scanner, while the back side (approximately Y > 15) is much sparser due to occlusion.\n",
    "\n",
    "## Challenges\n",
    "Several challenges arise when attempting to accurately count leaves and estimate leaf area from 3D point cloud data:\n",
    "\n",
    "1. **Data Quality**: The distances between points in the point cloud can vary significantly, leading to uneven point density. This is due to factors such as the diffrent sampling density between horizontal and vertical directions, distance from the LiDAR to the object (the further the object is, the sparser the points are). In general, the sparser the points are, the harder it is to accurately identify and segment individual leaves.\n",
    "\n",
    "2. **Occlusion**: Leaves can be occluded by other leaves or branches, making it difficult to accurately count them. This is especially true in dense canopies where leaves overlap significantly. Additionally, **this dataset is obtained from a single scan position, which can lead to significant occlusion issues**.\n",
    "\n",
    "3. **Leaf Size and Shape Variability**: Leaves can vary greatly in size and shape, making it challenging to develop a one-size-fits-all algorithm for leaf detection and area estimation.\n",
    "\n",
    "4. **Leaf Overlap**: In dense canopies, leaves often overlap each other, leading to hard-to-seperate clusters of points that may represent multiple leaves. This can lead to undercounting of leaves and inaccurate leaf area estimates.\n",
    "\n",
    "## Assumptions\n",
    "To address these challenges, we make several assumptions in our analysis. These assumptions also based on what we observe visually from the point cloud data:\n",
    "\n",
    "1. **Point Density**: The point cloud density is adequate to capture the essential structure of the leaves. Its resolution is finer than the most leaves' size.\n",
    "\n",
    "2. **Leaf Shape**: Leaves are mostly planar and can be approximated as flat surfaces for area estimation.\n",
    "\n",
    "3. **Environmental Conditions**: The data was obtained in still air conditions, minimizing motion blur or distortion in the point cloud.\n",
    "\n",
    "## Approach Summary\n",
    "Our method goes through several steps:\n",
    "\n",
    "1. Keep only the tree half which faces the LiDAR scanner. (to mitigate occlusion issues)\n",
    "\n",
    "2. Cluster the tree into smaller clusters.\n",
    "\n",
    "3. Use different clustering proposing methods **([DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), [RANSAC](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html) plane fitting)** to find the best clustering which gives the most leaf-like clusters for each smaller cluster. The leaf-like clusters are identified by **shape analysis**.\n",
    "\n",
    "4. Compute leaf area for each identified leaf cluster and sum them up to get the total leaf area. The leaves areas are aproximated by **half the surface area of the convex hull of the leaf clusters**, because we assume the leaves are flat surfaces.\n",
    "\n",
    "5. Only keep the leaves which have area below 0.15 m^2 to avoid suspiciously large clusters.\n",
    "\n",
    "6. Visualize the final result and **show CT scans along different axes, for qualitative evaluation.** Clusters which look like leaves should be detected as leaves, and different leaves should be separated in the CT scans.\n",
    "\n",
    "7. Using **simulated data from HELIOS++** for estimate the effect of occlusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ecaa38",
   "metadata": {},
   "source": [
    "# Model and Solutions\n",
    "\n",
    "## Leaf cluster identification\n",
    "The leaves in this dataset can be modeled by a set of points, sampled from a 2D plane in 3D space. The size of the leaves can vary, but they are generally small compared to the size of the tree. The clusters of points that represent leaves should therefore have the following characteristics:\n",
    "\n",
    "- **Planarity**: The points in a leaf cluster should lie approximately on a plane. Ofcause, a line will also be able to lie on a plane, so the cluster should forms a flat surface, not a line. There can be some error tolerance due to the leaf curvature but overall, the points should be close to a flat surface.\n",
    "\n",
    "- **Separable**: The cluster should be have certain density inside, and separated from other clusters by sparse regions. There is different in density on horizontal and vertical directions due to how the LiDAR scaner works, but on each leaf cluster, the horizontal density is uniform and the vertical density is uniform.\n",
    "\n",
    "- **Shape and Size**: The area and dimensions of the cluster should be within reasonable limits for a leaf. Clusters that are too large or too small can be discarded.\n",
    "\n",
    "And that is how we identify leaf clusters in our approach. **Branches** are usually have sticky shapes, so they should fail planarity test. **Trunks** are usually big, unsperable surfaces, although a very large trunk piece can be planar, it should be unseparable large surface.\n",
    "\n",
    "We use these algorithms to identify leaf clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ffd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor, LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "def analyze_cluster_shape(points):\n",
    "    \"\"\"Analyze the shape of a cluster of points using PCA.\"\"\"\n",
    "    if points.shape[0] < 3:\n",
    "        return 0.0, 0.0, 0.0  # not enough points to analyze shape\n",
    "    \n",
    "    # get linearality, planarity, sphericity\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(points)\n",
    "    eigenvalues = pca.explained_variance_\n",
    "    total_variance = np.sum(eigenvalues)\n",
    "    linearity = (eigenvalues[0] - eigenvalues[1]) / total_variance\n",
    "    planarity = (eigenvalues[1] - eigenvalues[2]) / total_variance\n",
    "    sphericity = eigenvalues[2] / total_variance\n",
    "    return linearity, planarity, sphericity\n",
    "\n",
    "def points_lie_in_a_plane_criteria(points, threshold=0.02):\n",
    "    \"\"\"\n",
    "        Check if the points lie approximately in a plane using linear regression.\n",
    "    \"\"\"\n",
    "    # fit a 3D line to the points using linear regression\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(points[:, :2], points[:, 2])\n",
    "    # all point should be within a certain distance to the line, threshold = 0.02\n",
    "    predicted_z = lm.predict(points[:, :2])\n",
    "    distances = np.abs(points[:, 2] - predicted_z)\n",
    "    if np.all(distances < threshold):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def cluster_is_leaf_like(cluster_points, min_points=10, max_points=80, \n",
    "                         min_ratio=20, plane_threshold=0.02):\n",
    "    \"\"\"\n",
    "        Determine if a cluster of points is leaf-like based on size and shape criteria.\n",
    "    \"\"\"\n",
    "    linearity, planarity, sphericity = analyze_cluster_shape(cluster_points)\n",
    "    ratio = planarity / (sphericity + 1e-7)\n",
    "    is_on_plane = points_lie_in_a_plane_criteria(cluster_points, threshold=plane_threshold)\n",
    "    if cluster_points.shape[0] >= min_points and\\\n",
    "        cluster_points.shape[0] <= max_points and\\\n",
    "        ratio >= min_ratio and is_on_plane:\n",
    "        return True, linearity, planarity, sphericity\n",
    "    return False, linearity, planarity, sphericity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbd26e",
   "metadata": {},
   "source": [
    "`analyze_cluster_shape` function: compute the shape characteristics of a cluster using PCA. This figure [\\[Moorthy et al, 2020\\]](https://ieeexplore.ieee.org/document/8889474) can explain the meaning of linearity (lambda_1), planarity (lambda_2), and sphericity (lambda_3). \n",
    "\n",
    "![Shape Analysis](./shape_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a88825",
   "metadata": {},
   "source": [
    "`points_lie_in_a_plane_criteria` function: even though PCA can give us planarity value, it does not guarantee that the points really lie on a plane. Many shapes can have high planarity value but the points do not lie on a plane, for example, a half-cylinder shape, with large diameter. So we use this function to check if the points really lie on a plane, with some error tolerance.\n",
    "\n",
    "Finally, we combine these two functions in `cluster_is_leaf_like` function to identify leaf clusters. Given a set of points, the function returns True if the cluster is leaf-like, otherwise False. The criteria for a leaf-like cluster are:\n",
    "\n",
    "1. The number of points in the cluster is between `min_points` and `max_points`. (based on observed leaf sizes in the point cloud)\n",
    "\n",
    "2. The ratio of planarity to sphericity is greater than `min_ratio`. (to ensure the cluster forms a flat surface, not a line or a blob)\n",
    "\n",
    "3. The points lie approximately on a plane, as determined by the `points_lie_in_a_plane_criteria` function.\n",
    "\n",
    "## Objective Function\n",
    "Now that we have a method to identify leaf clusters, we can define our objective function which by optimizing it, we can find the best clustering of our point cloud, which also includes semantic segmentation (differentiating wood from leaf) and instance segmentation (distinguishing individual leaves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb690a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(points, labels, min_points=10, max_points=80, min_ratio=20, plane_threshold=0.02):\n",
    "    \"\"\"\n",
    "        Receive some clustered points and their labels\n",
    "        and return a score based on quantity and quality of leaf-like clusters found\n",
    "    \"\"\"\n",
    "    leaf_cluster_id_2_stats = {}\n",
    "    for cluster_label in set(labels):\n",
    "        if cluster_label == -1:\n",
    "            continue  # skip noise\n",
    "        cluster_points = points[labels == cluster_label]\n",
    "        is_leaf, linearity, planarity, sphericity = cluster_is_leaf_like(cluster_points, \n",
    "                                                        min_points=min_points, \n",
    "                                                        max_points=max_points, \n",
    "                                                        min_ratio=min_ratio,\n",
    "                                                        plane_threshold=plane_threshold)\n",
    "        if is_leaf:\n",
    "            leaf_cluster_id_2_stats[cluster_label] = (linearity, planarity, sphericity)\n",
    "            \n",
    "    score = 0\n",
    "    for cluster_label, stats in leaf_cluster_id_2_stats.items():\n",
    "        planarity = stats[1]\n",
    "        num_points = np.sum(labels == cluster_label)\n",
    "        # score += num_points**2 * planarity\n",
    "        score += num_points**2\n",
    "    return score, leaf_cluster_id_2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59f837",
   "metadata": {},
   "source": [
    "As we can see in `score_function`, the **score is equal the sum of square of number of points in each leaf-like cluster**. This objective function encourages the clustering algorithm to produce clusters that are leaf-like and as large as possible (in terms of number of points), because we don't want out algorithm to segment leaves into many small clusters, that would cause over estimate the number of leaves and underestimate their sizes. That is how we **incoporated the leaf-like constraint into the objective function**. By maximizing this score, we can find a clustering that effectively segments the point cloud into individual leaves while minimizing the inclusion of non-leaf points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341d554",
   "metadata": {},
   "source": [
    "## Optimization Method\n",
    "\n",
    "To optimize the objective function, we experimented with different clustering algorithms to segment the point cloud into clusters. We used **DBSCAN** and **RANSAC plane fitting** as our main clustering methods. There is no sophisticated optimization algorithm used here, we simply apply **exhaustive search over different hyperparameters and clustering methods**, and select the one which gives the highest score according to our objective function.\n",
    "\n",
    "### Initial Clustering\n",
    "After removing all the ground points by Z thresholding, removing the back half of the tree which is mostly occluded from the LiDAR scanner, we first cluster the tree into smaller clusters using [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) with fixed parameters (eps=0.07, min_samples=5). We then optimize the clustering within each smaller cluster using different clustering methods and hyperparameters.\n",
    "\n",
    "![Initial Clustering](./initial_clustering.png)\n",
    "\n",
    "\n",
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dccfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "def DBSCAN_cluster_optimization(cluster_points, min_points=15, max_points=80, \n",
    "                                min_ratio=10, plane_threshold=0.02):\n",
    "    \"\"\"\n",
    "        Receive some clustered points and try different DBSCAN parameters to find the best clustering\n",
    "        that maximizes the score_function.\n",
    "    \n",
    "        Returns: best_score, best_num_leaf_clusters, clustering_assignment, leaf_label_set\n",
    "        \n",
    "        clustering_assignment: array of cluster labels for each point in cluster_points\n",
    "        leaf_label_set: set of cluster labels that are considered leaf-like\n",
    "        best_score: highest score achieved\n",
    "        best_num_leaf_clusters: number of leaf-like clusters found at best score\n",
    "    \"\"\"\n",
    "    best_score = -1\n",
    "    best_num_leaf_clusters = 0\n",
    "    clustering_assignment = -1 * np.ones(cluster_points.shape[0], dtype=int)\n",
    "    leaf_label_set = set()\n",
    "\n",
    "    list_eps = np.arange(0.1, 0.01, -0.005)\n",
    "    list_min_samples = [5, 10, 15, 20]\n",
    "    \n",
    "    for min_samples in list_min_samples:\n",
    "        for eps in list_eps:\n",
    "\n",
    "            scaled_points = cluster_points.copy()\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=16)\n",
    "            labels = dbscan.fit_predict(scaled_points)\n",
    "            \n",
    "            # # the number of points in -1 cluster should be less than 20% of total points\n",
    "            num_noise_points = np.sum(labels == -1)\n",
    "            if num_noise_points > 0.2 * scaled_points.shape[0]:\n",
    "                continue  # skip this scale\n",
    "            \n",
    "            # use ransac to remove points that are not on planes?\n",
    "            # for each cluster, fit a plane using ransac, keep only inlier points\n",
    "            for label in set(labels):\n",
    "                if label == -1: # don't process noise\n",
    "                    continue\n",
    "                cluster_points_ransac = scaled_points[labels == label]\n",
    "                if cluster_points_ransac.shape[0] < 3:\n",
    "                    continue\n",
    "                ransac = RANSACRegressor(residual_threshold=plane_threshold, \n",
    "                                         min_samples=3, max_trials=300)\n",
    "                X = cluster_points_ransac[:, :2]  # use x and y as input\n",
    "                y = cluster_points_ransac[:, 2]    # use z as output\n",
    "                ransac.fit(X, y)\n",
    "            \n",
    "                inlier_mask = ransac.inlier_mask_\n",
    "                cluster_points_ransac = cluster_points_ransac[inlier_mask]\n",
    "                # update labels\n",
    "                labels_indices = np.where(labels == label)[0]\n",
    "                for i, idx in enumerate(labels_indices):\n",
    "                    if not inlier_mask[i]:\n",
    "                        # remove outlier point\n",
    "                        labels[idx] = -1  # mark as noise\n",
    "            \n",
    "            score, leaf_cluster_id_2_stats = score_function(scaled_points, labels, \n",
    "                                                        min_points=min_points, \n",
    "                                                        max_points=max_points, \n",
    "                                                        min_ratio=min_ratio,\n",
    "                                                    plane_threshold=plane_threshold)\n",
    "            \n",
    "            if score > best_score or (np.abs(score - best_score) <= 5 and len(leaf_cluster_id_2_stats) > best_num_leaf_clusters):\n",
    "                best_score = score\n",
    "                best_num_leaf_clusters = len(leaf_cluster_id_2_stats)\n",
    "                leaf_label_set = set(leaf_cluster_id_2_stats.keys())\n",
    "                # outlier points are labeled as -1 \n",
    "                clustering_assignment = labels.copy()\n",
    "            \n",
    "    return best_score, best_num_leaf_clusters, clustering_assignment, leaf_label_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b8b40",
   "metadata": {},
   "source": [
    "Here, DBSCAN is choosen because it is able to find clusters of arbitrary shapes and sizes, and these clusters are sperated by sparse regions, which aligns well with our leaf cluster identification criteria. Here is a short explanation of DBSCAN: \n",
    "\n",
    "<img src=\"./DBSCAN-Illustration.png\" alt=\"drawing\" width=\"320\" height=\"231\"/>\n",
    "\n",
    "In this diagram, minPts = 4. Point A and the other red points are core points, because the area surrounding these points in an ε radius contain at least 4 points (including the point itself). Because they are all reachable from one another, they form a single cluster. Points B and C are not core points, but are reachable from A (via other core points) and thus belong to the cluster as well. Point N is a noise point that is neither a core point nor directly-reachable. [\\[wikipedia\\]](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "\n",
    "Also, in this algorithm, we use **RANSAC plane fitting** to refine the clusters found by DBSCAN. **RANSAC (RANdom SAmple Consensus)** can be used to find a planar fit for a set of 3D points, while being robust to outliers. In our case, we treat the x, y coordinates as inputs and z as the output to find a planar fit. Points within each cluster that are identified as outliers to this plane (based on a predefined threshold) are re-labeled as noise (-1). This step helps to further refine the clusters by removing points that do not conform to the planar structure expected of leaf surfaces. A short introduction to RANSAC can be found in [Fouhey, 2011](https://cs.nyu.edu/~fouhey/earlier/thesis/dfouhey_thesisPresentation.pdf).\n",
    "\n",
    "<img src=\"./sphx_glr_plot_ransac_001 (1).png\" alt=\"drawing\" width=\"640\" height=\"462\"/>\n",
    "\n",
    "image source: [sklearn RANSAC example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html)\n",
    "\n",
    "\n",
    "The `DBSCAN_cluster_optimization` function clustering the set of points following these steps:\n",
    "\n",
    "1. **Choose a set of parameter combinations** for `eps` and `min_samples` to try.\n",
    "2. **Apply DBSCAN**: For each parameter combination, run the DBSCAN algorithm on the cluster_points to generate initial cluster labels.\n",
    "3. **Initial Noise Filter**: Check the percentage of points classified as noise (label -1). If more than 20% of the total points are noise, discard this parameter set and move to the next iteration.\n",
    "4. **Planar Refinement (RANSAC)**: Iterate through each identified cluster and attempt to fit a plane to the points using `RANSACRegressor`. It treats the x, y coordinates as inputs and z as the output to find a planar fit.Points within the cluster that are identified as outliers to this plane (based on the plane_threshold) are re-labeled as noise (-1).\n",
    "5. **Scoring**: Calculate a fitness score for the current clustering using the `score_function`.\n",
    "6. **Optimization**: Compare the current score against the `best_score` recorded so far. **Update the best result if: The current score is strictly higher than the best score. OR the current score is within 5 points of the best score, but the current configuration yields a higher number of leaf clusters.**\n",
    "7. **Return**: Finally, return the configuration that produced the best score, including the cluster assignments, the count of leaf clusters, and the set of valid leaf label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e86d2",
   "metadata": {},
   "source": [
    "### RANSAC\n",
    "Another way to cluster the points is to first use RANSAC to find planar clusters in the point cloud. But the planes found by RANSAC are not bounded by point density, these planes will cut through multiple objects in the point cloud. Therefore, after finding planes using RANSAC, we further cluster the points within each plane using DBSCAN to separate different objects that lie on the same plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3793a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RANSAC_cluster_optimization(cluster_points, distance_threshold=0.01, \n",
    "                                min_inliers=20, dbscan_min_points=15, \n",
    "                                dbscan_max_points=80, dbscan_min_ratio=10, \n",
    "                                dbscan_plane_threshold=0.02):\n",
    "    \"\"\"\n",
    "        Sequential RANSAC to find leaf planes in 3D point clouds.\n",
    "        After finding a plane using RANSAC, use DBSCAN to separate different clusters within the plane.\n",
    "        If the clusters found by DBSCAN are leaf-like, keep them,\n",
    "            and remove the clusters which are not leaf-like (for the next iterations).\n",
    "        Repeat until no more planes can be found with enough inliers, \n",
    "            or no new inliers are found even after several iterations.\n",
    "    \"\"\"\n",
    "    remaining_points = cluster_points.copy()\n",
    "    leaf_planes = []\n",
    "    \n",
    "    list_len_remaining = []\n",
    "    while remaining_points.shape[0] >= min_inliers:\n",
    "        # fit RANSAC plane model\n",
    "        ransac = RANSACRegressor(residual_threshold=distance_threshold, \n",
    "                                 min_samples=3, max_trials=300)\n",
    "        X = remaining_points[:, :2]  # use x and y as input\n",
    "        y = remaining_points[:, 2]    # use z as output\n",
    "        ransac.fit(X, y)\n",
    "        \n",
    "        inlier_mask = ransac.inlier_mask_\n",
    "        inlier_points = remaining_points[inlier_mask]\n",
    "        \n",
    "        # there can be multiple cluster in inlier points so we want to separate them\n",
    "        # use DBSCAN to separate them, find the best DBSCAN clustering and remove good clusters.\n",
    "        best_score, best_num_leaf_clusters,\\\n",
    "            clustering_assignment,\\\n",
    "                leaf_label_set = DBSCAN_cluster_optimization(inlier_points,\n",
    "                                            min_points=dbscan_min_points, \n",
    "                                            max_points=dbscan_max_points, \n",
    "                                            min_ratio=dbscan_min_ratio,\n",
    "                                        plane_threshold=dbscan_plane_threshold)\n",
    "        \n",
    "        # # set inlier_points to only those points that are labeled as leaf-like clusters\n",
    "        for cluster_id in leaf_label_set:\n",
    "            cluster_points = inlier_points[clustering_assignment == cluster_id]\n",
    "            # if cluster_points.shape[0] >= min_inliers:\n",
    "            leaf_planes.append(cluster_points)\n",
    "                \n",
    "        new_inlier_mask = list()\n",
    "        inlier_points_pos = 0\n",
    "        for i in range(inlier_mask.shape[0]):\n",
    "            if inlier_mask[i]:\n",
    "                if clustering_assignment[inlier_points_pos] in leaf_label_set:\n",
    "                    new_inlier_mask.append(True)\n",
    "                else:\n",
    "                    new_inlier_mask.append(False)\n",
    "                inlier_points_pos += 1\n",
    "            else:\n",
    "                new_inlier_mask.append(False)\n",
    "        \n",
    "        # remove inliers from remaining points\n",
    "        outlier_mask = np.logical_not(new_inlier_mask)\n",
    "        remaining_points = remaining_points[outlier_mask]\n",
    "        \n",
    "        len_remaining = remaining_points.shape[0]\n",
    "        list_len_remaining.append(len_remaining)\n",
    "        # if 3 time and still no change, break\n",
    "        if len(list_len_remaining) >= 5:\n",
    "            if all(x == list_len_remaining[-1] for x in list_len_remaining[-3:]):\n",
    "                break\n",
    "    \n",
    "    return leaf_planes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e79c8",
   "metadata": {},
   "source": [
    "For comparision, these are the resulting clusters from DBSCAN and RANSAC methods on the same cluster of points:\n",
    "\n",
    "**Original Point Cloud:**\n",
    "\n",
    "![Original Point Cloud](./original_clus.png)\n",
    "\n",
    "\n",
    "**DBSCAN Clustering Result:**\n",
    "\n",
    "![DBSCAN Clustering](./dbscan_clus.png)\n",
    "\n",
    "\n",
    "**RANSAC Clustering Result:**\n",
    "\n",
    "![RANSAC Clustering](./ransac_clus.png)\n",
    "\n",
    "\n",
    "We can see that when the point cloud is dense with no clear separation between objects, DBSCAN tends to group multiple leaves into a single cluster. On the other hand, RANSAC is able to separate these leaves into different clusters by fitting planes and then using DBSCAN within each plane. But cutting a plane through multiple objects can also lead to over-segmentation, where a single leaf is split into multiple clusters if it is not perfectly planar, so we still keep both methods and choose the best one based on our objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc83c61",
   "metadata": {},
   "source": [
    "### Final cluster assignment optimization function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ccc62",
   "metadata": {},
   "source": [
    "The final optimization function then compare clutering results from both DBSCAN and RANSAC methods, and select the one which gives the highest score according to our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_for_cluster(cluster_points, ransac_only=False):\n",
    "    \"\"\"\n",
    "        This function return the best subclustering labels for the given cluster points\n",
    "        by trying both DBSCAN and RANSAC methods, and selecting the one with the best score.\n",
    "        1. Try DBSCAN to find subclusters\n",
    "        2. Try sequential RANSAC to find leaf planes\n",
    "        3. Return the best clustering assignment.\n",
    "    \"\"\"\n",
    "    # try DBSCAN first\n",
    "    best_score = -1\n",
    "    clustering_assignment = -1 * np.ones(cluster_points.shape[0], dtype=int)\n",
    "    leaf_label_set = set()\n",
    "    best_num_leaf_clusters = 0\n",
    "    if not ransac_only:\n",
    "        best_score, best_num_leaf_clusters, \\\n",
    "            clustering_assignment,\\\n",
    "            leaf_label_set = DBSCAN_cluster_optimization(cluster_points,\n",
    "                        min_points=param['dbscan']['min_points'], \n",
    "                        max_points=param['dbscan']['max_points'], \n",
    "                        min_ratio=param['dbscan']['min_ratio'], \n",
    "                        plane_threshold=param['dbscan']['plane_threshold'])\n",
    "    \n",
    "    # # Try RANSAC\n",
    "    best_leaf_planes = []\n",
    "    scaled_points = cluster_points.copy()\n",
    "    leaf_planes = RANSAC_cluster_optimization(scaled_points, \n",
    "                distance_threshold=param['ransac']['distance_threshold'], \n",
    "                min_inliers=param['ransac']['min_inliers'], \n",
    "                dbscan_min_points=param['ransac']['dbscan_min_points'], \n",
    "                dbscan_max_points=param['ransac']['dbscan_max_points'], \n",
    "                dbscan_min_ratio=param['ransac']['dbscan_min_ratio'],\n",
    "                dbscan_plane_threshold=param['ransac']['dbscan_plane_threshold'])\n",
    "    # score is total number of points in leaf planes\n",
    "    score = sum(plane.shape[0]**2 for plane in leaf_planes)\n",
    "    \n",
    "    if score > best_score or (np.abs(score - best_score) <= 5 and len(leaf_planes) > best_num_leaf_clusters):\n",
    "        best_score = score\n",
    "        best_num_leaf_clusters = len(leaf_planes)\n",
    "        best_leaf_planes = leaf_planes\n",
    "        \n",
    "        # translate best_leaf_planes to clustering_assignment\n",
    "        if len(best_leaf_planes) > 0:\n",
    "            clustering_assignment = -1 * np.ones(cluster_points.shape[0], dtype=int)\n",
    "            cluster_id = 0\n",
    "            leaf_label_set = set()\n",
    "            for plane in best_leaf_planes:\n",
    "                for point in plane:\n",
    "                    # find index of point in cluster_points\n",
    "                    indices = np.where((np.round(cluster_points, decimals=4) == np.round(point, decimals=4)).all(axis=1))[0]\n",
    "                    assert len(indices) > 0, \"Point from leaf plane not found in original cluster points\"\n",
    "                    for index in indices:\n",
    "                        clustering_assignment[index] = cluster_id\n",
    "                leaf_label_set.add(cluster_id)\n",
    "                cluster_id += 1\n",
    "        print(f\"After RANSAC: Best Score: {best_score}, Best Number of leaf-like clusters: {best_num_leaf_clusters}\")\n",
    "    else:\n",
    "         print(f\"After DBSCAN: Best Score: {best_score}, Best Number of leaf-like clusters: {best_num_leaf_clusters}\")\n",
    "                \n",
    "    return best_score, best_num_leaf_clusters, clustering_assignment, leaf_label_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fde938",
   "metadata": {},
   "source": [
    "Each cluster from initial clustering is processed independently, so the optimization can be parallelized for faster computation. The whole optimization process's duration is around 1.5 minutes on our personal computer with 16 CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def process_cluster(cluster_label, scaled_points, labels):\n",
    "    if cluster_label == -1:\n",
    "        return cluster_label, None  # skip noise\n",
    "    cluster_points = scaled_points[labels == cluster_label]\n",
    "    \n",
    "    best_score, best_num_leaf_clusters, clustering_assignment, leaf_label_set = optimize_for_cluster(cluster_points)\n",
    "    # print(f\"Cluster {cluster_label}: Best Score (leaf-like clusters): {best_score}, Number of leaf-like clusters: {best_num_leaf_clusters}\")\n",
    "    return cluster_label, (best_score, best_num_leaf_clusters, clustering_assignment, leaf_label_set)\n",
    "results = Parallel(n_jobs=16)(delayed(process_cluster)(cluster_label, scaled_points, labels) for cluster_label in set(labels))\n",
    "cluster_id_2_best_clustering = {cluster_label: result for cluster_label, result in results if result is not None}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee7a3c",
   "metadata": {},
   "source": [
    "You can also see that somethings very important is missing here. In `optimize_for_cluster` function, we use `param` but nowhere to define it. The `param` variable is a dictionary that contains the hyperparameters for the RANSAC and DBSCAN algorithms used in the clustering process. The hyperparameters in `param` are tuned by manually, based on visual inspection of the clustering results on a few sample clusters from the point cloud. The chosen hyperparameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ea13a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_1 = {\n",
    "    'dbscan':{\n",
    "        'min_points':12,\n",
    "        'max_points':70,\n",
    "        'min_ratio':20,\n",
    "        'plane_threshold':0.04,\n",
    "    },\n",
    "    'ransac':{\n",
    "        'distance_threshold':0.04,\n",
    "        'min_inliers':12,\n",
    "        'dbscan_min_points':12,\n",
    "        'dbscan_max_points':70,\n",
    "        'dbscan_min_ratio':20,\n",
    "        'dbscan_plane_threshold':0.04,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eab5ee",
   "metadata": {},
   "source": [
    "It is very hard to inspect the clustering results in 3D visually, so we use CT scans along different axes to evaluate the clustering quality. Clusters which look like leaves should be detected as leaves, and different leaves should be separated in the CT scans.\n",
    "\n",
    "In the below CT scans, you can see that most leaves are correctly identified and separated. \n",
    "\n",
    "- **Black dots** are noise points, seperated by the initial clustering\n",
    "- **Colored dots** are clusters which failed leaf-like criteria\n",
    "- **Colored triangles** are identified leaf clusters\n",
    "- **Gray faded dots** are occluded data, which we was able to uncovered by using HELIOS++ simulation.\n",
    "\n",
    "**CT scan along Z axis:**\n",
    "\n",
    "![CT scan Z axis](./ct_scan_z.png)\n",
    "\n",
    "**CT scan along Y axis:**\n",
    "\n",
    "![CT scan Y axis](./ct_scan_y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40909425",
   "metadata": {},
   "source": [
    "### Area Estimation and Leaf Counting\n",
    "\n",
    "After identifying the leaf clusters, we estimate the area of each leaf cluster using the convex hull method. The convex hull of a set of points is the smallest convex shape that encloses all the points. For a leaf cluster, we compute the convex hull of its points and calculate its surface area. Since leaves are generally flat surfaces, we **approximate the leaf area as half the surface area of the convex hull.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "leaf_areas = []\n",
    "leaf_largest_distances = []\n",
    "cluster_label_2_area = {}\n",
    "cluster_label_2_largest_distance = {}\n",
    "count = 0\n",
    "for cluster_label in set(new_labels):\n",
    "    if cluster_label == -1:\n",
    "        continue  # skip noise\n",
    "    \n",
    "    points = orig_points[new_labels == cluster_label]\n",
    "    is_leaf = is_leaf_label[new_labels == cluster_label]\n",
    "    \n",
    "    if not np.all(is_leaf):\n",
    "        continue  # not a leaf cluster\n",
    "    \n",
    "    # compute convex hull\n",
    "    try:\n",
    "        hull = ConvexHull(points)\n",
    "    except:\n",
    "        print(f\"Cluster {cluster_label} cannot find convex hull.\")\n",
    "        continue  # skip clusters that cannot be triangulated\n",
    "    \n",
    "    # compute largest distance between points in the cluster\n",
    "    from scipy.spatial.distance import pdist    \n",
    "    largest_distance = np.max(pdist(points))\n",
    "    cluster_label_2_largest_distance[cluster_label] = largest_distance\n",
    "    leaf_largest_distances.append(largest_distance)\n",
    "    \n",
    "    # compute area of each triangle\n",
    "    area = hull.area / 2.0  # Leaft area is half of the convex hull area\n",
    "    leaf_areas.append(area)\n",
    "    cluster_label_2_area[cluster_label] = area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e8c80",
   "metadata": {},
   "source": [
    "**A typical convex hull of a leaf cluster**\n",
    "\n",
    "![Convex Hull](./convex_hull.png)\n",
    "\n",
    "**Histogram of estimated leaf areas**\n",
    "\n",
    "![Leaf Area Histogram](./area_hist.png)\n",
    "\n",
    "**Histogram of largest distance between points in leaf clusters**\n",
    "![Largest Distance Histogram](./length_hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3813a0b",
   "metadata": {},
   "source": [
    "For the final leaf count and total leaf area estimation, we apply a threshold to filter out suspiciously large clusters that are likely to be spurious leaves. We set a maximum area threshold of 0.2 m^2; any leaf cluster with an estimated area above this threshold is discarded from the final count and area sum. We discard leaves with largest distance between points above 1 m for the same reason. The reason for these large thresholds is we alow cases where mutiple leaves are clustered together in dense regions, leading to larger clusters that still represent valid leaf structures.\n",
    "\n",
    "And this is how we **incorporated shape and size constraints** into our leaf counting and area estimation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3feb4f",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "For the final results, after filtering our spurious clusters, and multiply everything by 2, as our input data was only half of the tree, we estimated a **total of 916 leaves with a combined leaf area of approximately 55.59 m^2.**\n",
    "\n",
    "## Objective Evaluation\n",
    "As stated earlier, there is no ground truth information in this dataset, so we only rely on qualitative evaluation using CT scans and visual inspection of clustering results. \n",
    "\n",
    "However, to estimate the effect of occlusion, luckily, we have founded the same exact tree model in HELIOS++ dataset, we then scan it from multiple angles to obtain a more complete point cloud of the tree. In this way, we can compare the leaf count and area estimation results from our single-scan data against the more complete multi-scan data to assess the impact of occlusion on our estimates.\n",
    "\n",
    "**Position of scanner in HELIOS++ multi-scan data:**\n",
    "\n",
    "![HELIOS++ Scanner Position](./scanner_locations.png)\n",
    "\n",
    "**Compared to our single-scan data, the multi-scan data is denser and have duble the number of points:**\n",
    "\n",
    "![HELIOS++ Multi-scan Point Cloud](./multi_scan_compared.png)\n",
    "\n",
    "Rerun our leaf counting and area estimation pipeline on the HELIOS++ multi-scan data, we obtained a total of **1256 leaves with a combined leaf area of approximately 83.52 m^2.** \n",
    "\n",
    "So, **underestimation due to occlusion in our single-scan data is approximately: 27% for leaf count and 33% for leaf area.** The percentage of leaf count is lower than leaf area, because in casses where even half of a leaf is visible, our algorithm can still detect it as a leaf cluster, contributing to the leaf count, but the estimated area will be lower due to occlusion.\n",
    "\n",
    "This magnitude of underestimation seems reasonable given the significant occlusion observed in the single-scan point cloud. It highlights the importance of multi-angle scanning for more accurate leaf quantification in complex canopies. We can see there are a lot of occluded leaves (gray faded dots) in this CT scan:\n",
    "\n",
    "![Occluded Leaves](./missing_leaf.png)\n",
    "\n",
    "## Robustness\n",
    "\n",
    "There are many hand tuned hyperparameters in our method, from the initial clustering to the leaf-like cluster identification criteria. These hyperparameters are tuned based on visual inspection of clustering results on a few sample clusters from the point cloud. Therefore, the robustness of our method depends on how well these hyperparameters generalize to different tree structures and point cloud densities.\n",
    "\n",
    "\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b0cf3",
   "metadata": {},
   "source": [
    "We proposed a pipeline to identify leaves from a tree point cloud obtained by a terrestrial LiDAR scanner.\n",
    "\n",
    "The method go through several steps:\n",
    "1. Keep only the tree half which faces the LiDAR scanner.\n",
    "2. Cluster the tree into smaller clusters.\n",
    "3. For each smaller cluster, find the best clustering which gives the most leaf-like clusters. The clustering proposing methods are: **DBSCAN**; **RANSAC** plane fitting. The leaf-like clusters are identified by **shape analysis**. We use **exhaustive search over different hyperparameters and clustering methods**, and select the one which gives the highest score according to our **objective function**.\n",
    "4. We compute leaf area for each identified leaf cluster and sum them up to get the total leaf area. The leaves areas are aproximated by half the **area of the convex hull** of the leaf clusters, because we assume the leaves are flat surfaces.\n",
    "5. We only keep the leaves which have area below 0.2 m^2 and largest dimension below 1.0 m to **avoid suspiciously large clusters.**\n",
    "6. We visualize the final result and show **CT scans along different axes, for qualitative evaluation.**\n",
    "7. We report the total leaf area as our estimation of leaf area of the tree.\n",
    "8. We use **simulated data from HELIOS++** for estimate the effect of occlusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ede8b",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## The whole ipynb file\n",
    "You can check all the results and plots in this file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
